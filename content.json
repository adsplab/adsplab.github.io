{"posts":[{"title":"基于信息论理解和加强多意图口语理解中的意图识别和槽位填充的联合学习","text":"论文信息 Xianwei Zhuang#, Xuxin Cheng#, Yuexian Zou*. Towards Explainable Joint Models via Information Theory for Multiple Intent Detection and Slot Filling. The 38th Annual AAAI Conference on Artificial Intelligence (AAAI 2024). Vancouver, Canada. 2024/2/20-2024/2/27. PPT链接：https://pan.baidu.com/s/1ljU-3GFeapzqz6UvADgh1w?pwd=kslz 成果简介 多意图检测和槽填充（Multi-Intent SLU）的核心在于如何有效建模意图和槽之间的相互作用。针对联合模型设计的可解释性和优化方法，本工作创新地提出了跨任务信息增益（Cross-Task Information Gain，CIG）概念。CIG是一种从信息论角度量化联合过程质量的理论度量，揭示了先前模型中隐含的CIG优化。基于此，我们提出了一个新颖的多阶段迭代框架，能够显式地优化跨任务交互的信息。进一步地，我们设计了一个基于信息的联合模型（InfoJoint），遵循这一理论框架，通过迭代最大化CIG逐渐减少联合任务中错误语义的传播。在两个公共数据集上的广泛实验结果表明，InfoJoint在所有评估指标上都大幅度超越了最先进的模型。","link":"/2024/11/06/%E5%BA%84%E5%85%88%E7%82%9C-AAAI-2024/"},{"title":"基于原型校准和师生解耦实现ASR噪声鲁棒的口语理解","text":"论文信息 Xianwei Zhuang, Xuxin Cheng, Liming Liang, Yuxin Xie, Zhichang Wang, Zhiqi Huang, Yuexian Zou*. PCAD: Towards ASR-Robust Spoken Language Understanding via Prototype Calibration and Asymmetric Decoupling. The 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), Bangkok, Thailand, 2024/8/11-2024/8/16. PPT链接：https://pan.baidu.com/s/19IERA9KDyS9vbbYib5ceLA?pwd=fxwo 成果简介 口语理解的核心在于如何有效处理自动语音识别（ASR）中的错误传播问题。针对ASR鲁棒性，本工作创新地提出了原型校准和非对称解耦（PCAD）框架。PCAD通过原型基损失来聚合标签和预测先验，校准偏差和错误倾向的语义，以实现更好的类间区分和类内一致性。理论上分析了这种损失对鲁棒性增强的效果。进一步地，本工作利用教师-学生模型进行非对称解耦训练，并提出了一种新颖的梯度敏感指数移动平均（GS-EMA）算法，以适应性地平衡准确性和鲁棒性。在三个数据集上的实验表明，PCAD显著优于现有方法，并实现了新的最先进性能。","link":"/2024/11/06/%E5%BA%84%E5%85%88%E7%82%9C-ACL2024/"},{"title":"一种用于视频文本检索的知识解耦概率框架。","text":"论文信息Xianwei Zhuang, Hongxiang Li, Xuxin Cheng, Zhihong Zhu, Yuxin Xie, Yuexian Zou* KDProR: A Knowledge-Decoupling Probabilistic Framework for Video-Text Retrieval. The 18th European Conference on Computer Vision (ECCV 2024), MiCo Milano, Italy, 2024/9/29-2024/10/4. 代码链接 mengchuang123/KDProR PPT链接：https://pan.baidu.com/s/1qBHxiMZfRhaRRT6zr6X-pQ?pwd=lxg9 成果简介 视频-文本检索（VTR）的核心在于如何有效设计跨模态交互机制。针对现有方法与人类学习范式的差异，本工作创新性地提出了一个知识解耦的概念，并构建了一个多尺度知识存储系统。本工作提出了KDProR框架，通过期望-知识最大化（EKM）算法进行优化。在E步中，KDProR从知识存储中获取相关上下文语义，并通过插值和对齐校正实现高效知识注入。在K步中，通过索引获得的Top-K知识来计算知识KNN分布，以校准检索分布；在M步中，通过最大化目标的似然来优化检索模型。在多个基准测试中，KDProR显著优于以往的最先进方法，特别地，KDProR能够统一且高效地整合多样的开放世界知识，并且与不同的交互机制和架构兼容。","link":"/2024/11/06/%E5%BA%84%E5%85%88%E7%82%9C-ECCV2024-KDProR/"},{"title":"用于弱监督时序行为定位的深度运动先验","text":"论文信息: Meng Cao, Can Zhang, Long Chen, Mike Zheng Shou, Yuexian Zou*. Deep Motion Prior for Weakly-Supervised Temporal Action Localization. IEEE Transactions on Image Processing, 2022 (31): 5203-5213. PPT链接：https://pan.baidu.com/s/19oqMe-w4Yx82lSG2JXF0gA?pwd=bgbu 成果简介: 弱监督时序行为定位（Weakly-Supervised Temporal Action Localization, WSTAL）旨在定位仅具有视频级标签的未修剪视频中的动作。目前，大多数最先进的WSTAL方法都采用多实例学习（Multi-Instance Learning, MIL）管道：首先生成片段级预测，然后汇总成视频级预测。然而，我们认为现有的方法存在两个缺点：1）运动信息利用不足，2）流行的交叉熵训练损失的不兼容性。本项研究分析了光流特征背后的运动线索是互补的信息。受此启发，我们建议建立一个与上下文相关的运动先验，称为运动性（motionness）。具体而言，我们引入了一个运动图，根据局部运动载体（如光流）对运动性进行建模。此外，为了突出信息量更大的视频片段，我们还提出了一种运动引导损失，用于调节以运动性得分为条件的网络训练。广泛的消融研究证实，运动性有效地模拟了感兴趣的动作，运动引导损失能带来更准确的结果。此外，我们的运动引导损耗是一种即插即用的代价函数，适用于现有的WSTAL方法。在不失一般性的情况下，基于标准MIL管道，我们的方法在三个具有挑战性的基准，THUMOS’14、ActivityNet v1.2和v1.3，上取得了领先性能。 演示视频1： 演示视频2：","link":"/2024/11/06/%E6%9B%B9%E8%92%99-TIP22-%E6%97%B6%E5%BA%8F%E8%A1%8C%E4%B8%BA%E5%AE%9A%E4%BD%8D/"},{"title":"基于局部-全局时序聚合的高效运动表征学习","text":"论文信息: Can Zhang, Yuexian Zou, Guang Chen, Lei Gan. PAN: Persistent Appearance Network with an Efficient Motion Cue for Fast Action Recognition. Proceedings of the 27th ACM International Conference on Multimedia. 2019: 500-509. 代码链接: zhang-can/PAN-PyTorch PPT链接: https://pan.baidu.com/s/12L7ITiPY1fu0ADe4aewnJA?pwd=lw6s 成果简介: 视频理解的核心在于如何有效建模运动信息。针对局部运动建模，本工作创新地提出了外观暂留（Persistence of Appearance，PA）概念。PA是一种可以替代光流的高效局部运动表征，通过对浅层特征空间中相邻帧特征的同通道进行一阶前向差分运算获得，可建模运动边界小位移。相比光流（Optical Flow）运动表征提取，PA表征的提取更高效，可提速1000倍；针对全局运动建模，本工作提出多时间尺度聚合，基于多时间尺度注意力机制，自适应地对特征重要性进行再分配和全局时序聚合。结合局部和全局运动建模，本工作在主流行为识别数据集上取得领先性能的同时，摆脱了光流依赖，具有计算复杂度低和运行效率高特性。 演示视频：","link":"/2024/11/06/%E5%BC%A0%E7%B2%B2-MM2019-PAN/"},{"title":"前白平衡网络：通过大气光估计在单张图像去噪中校正色偏","text":"论文信息: Cong Wang, Yan Huang, Yuexian Zou*, Yong Xu. FWB-Net: Front White Balance Network for Color Shift Correction in Single Image Dehazing via Atmospheric Light Estimation. The 46th International Conference on Acoustics, Speech and Signal Processing. 2021: 2040-2044. 成果简介: 近年来，基于大气散射模型（Atmospheric Scattering Model, ASM）的单张图像去噪深度神经网络取得了显著成果。但是这些网络的去噪输出会受到颜色偏移（Color Shift）的影响。通过分析ASM模型，我们发现颜色偏移是由大气光因子被设置为标量导致，该设置假设了大气光因子在整个图像中是恒定的。然而，对于在现实世界中拍摄的图像，光照并不是均匀分布在整个图像上的。考虑到这一点，我们在这项研究中首先提出了一种新的非均匀大气散射模型（NH-ASM），用于改进在复杂光照条件下拍摄的朦胧图像的图像建模；其次，我们专门设计了一种新的基于U-Net的前白平衡模块（FWB模块）和鼓励色偏校正的损失函数，用于在通过大气光估计生成去晕结果之前校正色偏。最后，基于NH-ASM和前白平衡技术，我们开发了FWB-Net，一种端到端的基于CNN的色偏抑制去噪网络。实验结果证明了我们提出的FWB-Net在合成图像和真实图像去噪方面的有效性和优越性。 演示视频：","link":"/2024/11/06/%E7%8E%8B%E8%81%AA-ICASSP21-%E5%8E%BB%E9%9B%BE/"},{"title":"概念感知视频描述：用有效的先验信息描述视频","text":"论文信息：Bang Yang, Meng Cao, Yuexian Zou. Concept-Aware Video Captioning: Describing Videos with Effective Prior Information. IEEE Transactions on Image Processing, 2023 (32): 5366-5378. 代码链接：yangbang18/CARE 成果简介：该工作聚焦视觉-语言交叉领域研究中具有挑战性的任务——视频描述（Video Captioning），即用流畅的自然语言文本来准确地描述视频中的内容。在这个任务中，概念（Concept），作为与视频内容中的对象（Object）、行为（Action）和属性（Attribute）相对应的有意义单词的统称，无疑起着重要作用。这是因为概念既是视频内容高层语义的体现，同时也能以文本的形式表示，是一种桥接视觉和文本模态的重要媒介。因此，如下框图所示，该工作提出概念感知视频描述模型CARE，旨在从视频中准确地检测出概念，从而为文本生成过程提供有效的先验信息。 所提CARE具有以下核心组件： 多模态驱动的概念检测（MCD）：与传统的视觉驱动的概念检测相比，MCD考虑了除视觉外的其他模态信息，即视频的音频信息和通过跨模态检索获得的视觉相关的文本信息。这些互相补充的多模态信息使得概念检测的精度（mAP）获得了2.1%~6.6%的绝对提升。 全局-局部语义引导（GSG &amp; LSG）：在概念利用方面，与主流做法不同，所提方法既影响了文本生成过程的全局偏好，也为逐单词预测的局部过程提供了语义引导。 混合注意力（HA）：在逐单词预测过程中，如何让模型利用恰当的视频信息和检测到的概念是一大关键。HA通过一次注意力操作即可让模型自适应地关注两种信息，在保持轻量的同时有效促进了局部语义引导。 对比式视觉-语言预训练模型CLIP：前期工作发现CLIP这类在隐空间中对齐图像和文本表征的预训练模型相比在ImageNet上预训练的分类模型学到了更具可区分性的视觉表征，可以给视频描述任务带来巨大提升。在本工作中，除了将CLIP应用于视觉编码，CARE还会利用CLIP的视觉-语言跨模态检索能力来辅助MCD，借此来最大程度地迁移CLIP的内部知识。 成果展示 以上视频展示了4个示例，其中每个例子展示了： CARE对于10s视频的描述生成结果，准确的关键词用黄色高亮突显； CARE的top 30概念检测结果（从左到右，从上到下排列）； CARE描述生成过程中对于检测到的概念的注意力热图（颜色越深，关注度越高）； CARE利用CLIP进行跨模态检索得到的检索文本（该检索结果被用于概念检测）。 总的来说，CARE擅长识别与视频场景相关的概念，并能够准确描述视频内容的细粒度细节。示例3和示例4也揭示了CARE的潜在改进方向：识别和利用视频中嵌入的文字。","link":"/2024/11/06/%E6%9D%A8%E9%82%A6-TIP2023-CARE/"},{"title":"面向视频异常检测的聚类注意力对比","text":"论文信息: Ziming Wang, Yuexian Zou*, Zeming Zhang. Cluster Attention Contrast for Video Anomaly Detection. Proceedings of the 28th ACM International Conference on Multimedia. 2020: 2463-2471. 成果简介: 视频异常检测（Video Anomaly Detection）通常是指对不符合预期行为的事件进行判别。现有的大多数方法都将视频异常检测视为离群点检测任务，并通过最小化训练数据的重构损失或预测损失来学习什么是常态（Normality）。然而，当这些方法无法保证对异常事件有较高的重构误差或对常态事件有较低的预测误差时，其性能就会下降。为了避免这些问题，我们引入了一种新颖的对比性表征学习任务——聚类注意对比。具体来说，我们采用多并行投影层将片段级视频特征投影到多个可区分的特征空间中。每个特征空间都对应一个集群，分别捕捉不同的常态子类别。为了获得可靠的子类别，我们提出了聚类注意力模块来提取每个视频片段的集群注意表示，然后通过动量对比，最大化同一视频片段在随机数据增强下的表示的一致性。通过这种方式，我们建立了一个稳健的关于常态的概念，而无需事先假设重建误差或预测误差。实验表明，我们的方法在基准数据集上取得了先进的性能。 演示视频：","link":"/2024/11/06/%E7%8E%8B%E5%AD%90%E9%93%AD-MM20-%E8%A7%86%E9%A2%91%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"}],"tags":[{"name":"口语理解","slug":"口语理解","link":"/tags/%E5%8F%A3%E8%AF%AD%E7%90%86%E8%A7%A3/"},{"name":"ASR噪声鲁棒的口语理解","slug":"ASR噪声鲁棒的口语理解","link":"/tags/ASR%E5%99%AA%E5%A3%B0%E9%B2%81%E6%A3%92%E7%9A%84%E5%8F%A3%E8%AF%AD%E7%90%86%E8%A7%A3/"},{"name":"多模态表征","slug":"多模态表征","link":"/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%A1%A8%E5%BE%81/"},{"name":"视频文本表征","slug":"视频文本表征","link":"/tags/%E8%A7%86%E9%A2%91%E6%96%87%E6%9C%AC%E8%A1%A8%E5%BE%81/"},{"name":"弱监督时序行为定位","slug":"弱监督时序行为定位","link":"/tags/%E5%BC%B1%E7%9B%91%E7%9D%A3%E6%97%B6%E5%BA%8F%E8%A1%8C%E4%B8%BA%E5%AE%9A%E4%BD%8D/"},{"name":"深度运动先验","slug":"深度运动先验","link":"/tags/%E6%B7%B1%E5%BA%A6%E8%BF%90%E5%8A%A8%E5%85%88%E9%AA%8C/"},{"name":"运动引导损失","slug":"运动引导损失","link":"/tags/%E8%BF%90%E5%8A%A8%E5%BC%95%E5%AF%BC%E6%8D%9F%E5%A4%B1/"},{"name":"运动表征学习","slug":"运动表征学习","link":"/tags/%E8%BF%90%E5%8A%A8%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/"},{"name":"行为识别","slug":"行为识别","link":"/tags/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB/"},{"name":"图像去噪","slug":"图像去噪","link":"/tags/%E5%9B%BE%E5%83%8F%E5%8E%BB%E5%99%AA/"},{"name":"色偏校正","slug":"色偏校正","link":"/tags/%E8%89%B2%E5%81%8F%E6%A0%A1%E6%AD%A3/"},{"name":"前白平衡","slug":"前白平衡","link":"/tags/%E5%89%8D%E7%99%BD%E5%B9%B3%E8%A1%A1/"},{"name":"视频描述","slug":"视频描述","link":"/tags/%E8%A7%86%E9%A2%91%E6%8F%8F%E8%BF%B0/"},{"name":"概念检测","slug":"概念检测","link":"/tags/%E6%A6%82%E5%BF%B5%E6%A3%80%E6%B5%8B/"},{"name":"检索增强生成","slug":"检索增强生成","link":"/tags/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90/"},{"name":"视频异常检测","slug":"视频异常检测","link":"/tags/%E8%A7%86%E9%A2%91%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"},{"name":"对比学习","slug":"对比学习","link":"/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"},{"name":"无监督学习","slug":"无监督学习","link":"/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"}],"categories":[{"name":"视觉组","slug":"视觉组","link":"/categories/%E8%A7%86%E8%A7%89%E7%BB%84/"}],"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"实验室介绍","text":"ADSPLAB实验室成立于2006年，位于北京大学深圳研究生院A306，拥有100平方米的实验室空间，实验室主任为邹月娴教授。实验室为信息工程学院开设全院选修课2门，分别为“模式识别导论”和“机器学习及其应用”。实验室自2006年来，已经培养北京大学全日制硕士研究生61名，承担国家科技项目4项（国家自然科学基金项目和863项目）、深圳市基础研究和技术创新项目13项、公司委托项目13项. 迄今，在多媒体信息技术处理领域形成了稳定的研究方向和一系列创新成果。实验室已购买音视频数据库、先进的摄像机设备、麦克风阵列和数据采集设备、语音和语言处理设备、GPU和服务器机群等，为开展相关研究奠定了基础。目前实验室主要致力于模式识别与机器学习及其应用、稀疏表示理论及其应用、听觉感知与语者确认理论与应用研究。 实验室成员导师：邹月娴 教授、博导 博士后：Asif Raza、张军斌 行政助理：张丽萍 在实验室研究生（24人）： 2024级博士研究生（1人）：尹永康 2024级硕士研究生（5人）：茹靖涵、唐乐翔、洪浩荣、尹雨果、胡志远 2023级博士研究生（1人）：梁立名 2023级硕士研究生（7人）：王之畅、庄先炜、谢宇昕、徐晚诗、陈展鹏、林义凯、谭凤 2022级博士研究生（2人）：杨绪升、李耀伟 2022级硕士研究生（7人）：程旭欣、朱志宏、姚子裕、李鸿翔、谭淑敏、聂宸涞、王信超 2021级博士研究生（1名）：杨邦 2024届硕士毕业生（7名）：蒋吉、曹博文、招梓枫、辛逸飞、王雯、宋腾韬、叶其琛 2024年ADSP硕士研究生毕业情况 2023届硕士/博士毕业生（10名）：曹蒙、田晋川、杨东超、叶忠杰、赵怡雯、吉普照、胡帅、安浩、陈东升、徐伟元 2023年ADSP硕士/博士研究生毕业情况 2022届硕士/博士毕业生（15名）：顾容之、张粲、杨东明、黄芝琪、李善浩、张皓然、陈诺、宁苒宇、陈立崧、吴立渝、王赫麟、王力、刘峰林、周子原、熊超 2022年ADSP硕士/博士研究生毕业情况 2021届硕士毕业生（7名）：周培林、王子铭、柳军领、王聪、洪思欣、张钰莹、侯晓龙 2021年 ADSP硕士研究生毕业情况 2020届硕士毕业生（7名）：陈广，刘钊祎，李子睿，甘蕾，王国帅，彭俊逸，蒲璐汶 2020年ADSP硕士研究生毕业情况 2019届硕士毕业生（6名）：关文婕，陆超豪，陈泽晗，罗丹青，种大丁，刘超 2018届硕士毕业生（5名）：王迪松，周小群，王宝岩，宋晓，张小虎，黄艺驰 2017届硕士毕业生（5名）：陈锦，柳俊宏，王毅，金彦含，黄晓林 2016届硕士毕业生（5名）：郑炜乔，向志强，王春，余嘉胜，刘诗涵 2015届硕士毕业生（5名）：郭轶凡，宁洪珂， 王永庆，夏德胜，张晓宇 2014届硕士毕业生（4名）：胡旭琰，李磊，马焘，余渊善 2013届****硕士毕业生（4名）：霍佳森，石伟，王鹏，徐祥俊 2012届硕士毕业生（5名）： 李波，任梦琪，何伟，姜鹏飞，吴继忠 2011届硕士毕业生（3名）：张尚良，霍然，陈昳丽 2010届硕士毕业生（6名）：郑亚莉，陈萧，施行，赵鹤，李培，崔蕾 2009届硕士毕业生（10名）：王伟东，王一言，赖晓强，陶阁，朱志东，任朝利，万波，关佩，杨华，冯国钊 2008届硕士毕业生（9名）：付洁，赵璟，吴天瑞，华扬，王巍，肖士杰，朱慧莹，向军，刘伟 研究项目承担的主要项目列表（按时间顺序）（projects listed in chronological order） 公司委托项目：“混合语音分离深度模型研究”（2020.5-2020.7，进行中） 深圳市产学研项目：“人体行为识别与检测关键技术”（2020.4-2021.12，进行中） 公司委托项目：“基于深度学习的音频分析方法与应用研究”（2019.11-2020.4，进行中） 深圳市基础研究布局项目：“基20180092智慧家庭服务机器人声学场景深度分析方法研究”（2019.3-2022.3，进行中） 公司委托项目：“基于深度学习的场景智能认知关键技术”（2018.5-2019.5，进行中） 公司委托项目：“音视频信息处理技术开发”（2018.7，进行中） 深圳市知识创新计划基础研究项目：“自然环境中多模态儿童情感智能感知关键技术研究”（2017-2019，结题） 深圳市科技计划项目：“面向VR/AR的听觉感知与交互关键技术研究”（2018.1-，进行中） 公司委托项目：“鸟类智能识别分析仪”（2017.9-，进行中） 公司委托项目：“基于机器学习的墨迹色阶自动采集方法研究”（2018，已结题） 公司委托项目：“基于声矢量信号的前端音频处理技术研究” （2017，已结题） 公司委托项目：“环视影像及人脸识别”（2016-2017 ，已结题） 深圳市基础研究布局项目：“基于动态视频数据的滨海湿地鸟类生态健康监测与评估研究”（2016.6.30-2019.6.30，结题） 深圳市基础研究计划项目：“无人机航拍视频图像处理关键技术研究”（2015-2017，已结题，项目编号：JCYJ20150430162332418） 深圳市技术创新计划项目：“远程非接触话者身份确认系统开发”（2014-2016，已结题，项目编号：CXZZ20140509093608290） 公司委托项目：“车型识别关键技术研究-基于局部特征和稀疏表示的车型识别”（2014-2015，已结题） 国家自然科学基金项目：“基于声学矢量传感器阵列和稀疏表示的语音声源方位角估计方法研究” （已结题，项目编号：61271309） 深圳市基础研究项目：“面向肠道疾病预防的无线胶囊内窥镜图像数据的智能分析与处理关键技术研究” （2013-2015，已结题） 大学委托项目：“基于机器学习的分类优化软件开发”（2015，已结题） 公司委托项目：“基于Deep Learning算法的特征识别”（2014，已结题） 深圳市基础研究计划：“基于压缩感知的多声源高精度定位方法研究”（2012-2014，已结题） 深圳市产学研项目：“基于云技术的区域电子健康档案智能服务系统”（2012-2013，已结题） 大学委托项目：“三维及多视角图像和视频的压缩和处理技术”（2011-2013，已结题） 公司委托项目：“无线胶囊内窥镜计算机辅助设计自动检测关键技术研发”（2011-2013，已结题） 公司委托项目：“嘈杂语音环境中的目标语音降噪录音设备”（2009-2011，已结题） 深圳市科技项目： “基于高精度多通道数字采集的体感诱发电位记录仪,” （2010，已结题） 研究所委托项目：“木材钻蛀性害虫声学特征研究”，（2009-2010，结题） 国家自然科学基金项目：“时间交替模数转换器（TIADC）通道失配误差数字补偿技术研究,” （已结题，项目编号：60775003） 国家科技部863项目：“基于多源信息融合的交通事件自动检测技术”（已结题，项目编号：2007AA11Z224） 深圳市科技计划项目： “交叉路口交通事件,” （2007-2008，已结题） 深圳市南山区科技计划项目：“基于麦克风阵列的实时自适应波束形成语音增强系统研究” （2006-2007，已结题） 国家科技部863项目：“面向HRI的机器人视听觉注意机制及运动规划技术”（已结题，项目编号：2006AA04Z247） 实验室课程实验室课程内容目前仅对北京大学深圳研究生院选课学生开放，登录密码由助教提供。 请点击进入相应课程，进入资料下载页面： 《模式识别》 《机器学习及应用》","link":"/about/index.html"}]}